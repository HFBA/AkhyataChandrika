#!/usr/bin/env python3
"""
Script to refresh existing redistributed work files with updated data.

Location: Scripts/AI_Generated/scripts/refreshAndRedistribute.py

This script:
1. Runs both collector scripts to refresh the output files with current counts
2. Redistributes the work into EXISTING part files in the existing folders

Usage (from project root):
    python3 Scripts/AI_Generated/scripts/refreshAndRedistribute.py \\
        Scripts/output/AkhyataChandrika_Autogenerated.json \\
        Scripts/AI_Generated/output

The script will detect existing folders and their part files, and refresh them with current data.

Arguments:
    json_file: Path to the generated JSON file
    output_dir: Directory where output files are located
"""

import json
import yaml
import sys
import os
import subprocess
import argparse
import math
from collections import OrderedDict
from pathlib import Path

# Custom YAML dumper to preserve strings and formatting
class QuotedDumper(yaml.SafeDumper):
    pass

def quoted_str_representer(dumper, data):
    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='"')

def ordered_dict_representer(dumper, data):
    return dumper.represent_dict(data.items())

yaml.add_representer(str, quoted_str_representer, Dumper=QuotedDumper)
yaml.add_representer(OrderedDict, ordered_dict_representer, Dumper=QuotedDumper)


def run_collectors(json_file, output_dir):
    """Run both collector scripts to refresh the output files."""
    print("=" * 70)
    print("STEP 1: Running collector scripts to refresh files...")
    print("=" * 70)

    scripts_dir = Path(__file__).parent / "collectors"

    # Run collectMultipleDhatuIds.py
    print("\nüìä Running collectMultipleDhatuIds.py...")
    script1 = scripts_dir / "collectMultipleDhatuIds.py"
    result1 = subprocess.run(
        ["python3", str(script1), json_file, output_dir],
        capture_output=True,
        text=True
    )

    if result1.returncode != 0:
        print(f"‚ùå Error running collectMultipleDhatuIds.py:")
        print(result1.stderr)
        return False

    # Run collectNotFoundDhatuIds.py
    print("\nüìä Running collectNotFoundDhatuIds.py...")
    script2 = scripts_dir / "collectNotFoundDhatuIds.py"
    result2 = subprocess.run(
        ["python3", str(script2), json_file, output_dir],
        capture_output=True,
        text=True
    )

    if result2.returncode != 0:
        print(f"‚ùå Error running collectNotFoundDhatuIds.py:")
        print(result2.stderr)
        return False

    print("\n‚úÖ Collector scripts completed successfully!")
    return True


def load_yaml_file(yaml_file):
    """Load a YAML file and return its data."""
    with open(yaml_file, 'r', encoding='utf-8') as f:
        # Skip comment lines
        lines = f.readlines()
        content = ''.join([line for line in lines if not line.strip().startswith('#')])
        return yaml.safe_load(content) or OrderedDict()


def load_existing_part_file(yaml_file):
    """Load an existing part file and preserve resolved status and comments."""
    if not os.path.exists(yaml_file):
        return {}

    data = load_yaml_file(yaml_file)
    # Return a dict mapping verb keys to their resolved/comment status
    resolved_status = {}
    for key, value in data.items():
        if isinstance(value, dict):
            resolved_status[key] = {
                'resolved': value.get('resolved', 'false'),
                'comment': value.get('comment', '')
            }
    return resolved_status


def write_redistributed_file(yaml_file, data, file_type, part_num, total_parts, total_items, existing_status=None):
    """Write a redistributed YAML file with proper formatting."""
    with open(yaml_file, 'w', encoding='utf-8') as f:
        # Add header comment
        f.write(f"# Cases where a verb has more than one dhatu_id\n")
        f.write("# Format: Each entry shows the verb form with its multiple dhatu_ids\n")
        f.write("# Manually edit this file to select the correct dhatu_id for each case\n")
        f.write("# After editing, run the backport script to sync changes back to original YAML files\n")
        f.write(f"# ENTRIES TO CORRECT: {len(data)}\n")
        f.write(f"# This is part {part_num} of {total_parts} - Assigned for proofreading\n\n")

        # Enhance data with resolved and comment fields
        enhanced_data = OrderedDict()
        for key, value in data.items():
            enhanced_value = OrderedDict(value)

            # Preserve existing resolved status and comments if available
            if existing_status and key in existing_status:
                enhanced_value['resolved'] = existing_status[key]['resolved']
                enhanced_value['comment'] = existing_status[key]['comment']
            else:
                enhanced_value['resolved'] = 'false'
                enhanced_value['comment'] = ''

            enhanced_data[key] = enhanced_value

        yaml.dump(
            enhanced_data,
            f,
            allow_unicode=True,
            default_flow_style=False,
            indent=2,
            sort_keys=False,
            width=1000,
            Dumper=QuotedDumper
        )


def collect_resolved_items(redistrib_dir):
    """
    Scan all existing part files and collect items that have been resolved.
    These items should NOT be re-added when refreshing.
    """
    resolved_items = set()

    if not redistrib_dir.exists():
        return resolved_items

    for part_file in redistrib_dir.glob("part_*.yaml"):
        data = load_yaml_file(part_file)
        for key, value in data.items():
            if isinstance(value, dict) and value.get('resolved') == 'true':
                resolved_items.add(key)

    return resolved_items


def redistribute_file(input_file, output_dir, file_basename, file_type):
    """Redistribute a single YAML file into existing part files."""

    if not os.path.exists(input_file):
        print(f"‚ö†Ô∏è  Warning: {input_file} not found, skipping...")
        return 0

    print(f"\nüìÇ Processing: {os.path.basename(input_file)}")

    # Check if redistrib directory exists
    redistrib_dir = Path(output_dir) / file_basename
    if not redistrib_dir.exists():
        print(f"  ‚ö†Ô∏è  Directory {redistrib_dir} not found, skipping...")
        return 0

    # Count existing part files to determine number of parts
    existing_parts = sorted(redistrib_dir.glob("part_*.yaml"))
    if not existing_parts:
        print(f"  ‚ö†Ô∏è  No part files found in {redistrib_dir}, skipping...")
        return 0

    num_parts = len(existing_parts)
    print(f"  üìä Found {num_parts} existing part files")

    # Collect items that have been resolved (should not be re-added)
    resolved_items = collect_resolved_items(redistrib_dir)
    if resolved_items:
        print(f"  ‚úì Found {len(resolved_items)} resolved items (will be skipped)")

    # Load the data from collector output
    data = load_yaml_file(input_file)

    if not data:
        print(f"  ‚ö†Ô∏è  Collector file is empty, skipping...")
        return 0

    # Filter out resolved items from the new data
    filtered_data = OrderedDict()
    skipped_count = 0
    for key, value in data.items():
        if key not in resolved_items:
            filtered_data[key] = value
        else:
            skipped_count += 1

    if skipped_count > 0:
        print(f"  ‚è≠Ô∏è  Skipped {skipped_count} already-resolved items")

    total_items = len(filtered_data)
    print(f"  üìä Total items to redistribute: {total_items}")

    if total_items == 0:
        print(f"  ‚úÖ All items have been resolved! Writing empty part files...")

    items_per_part = math.ceil(total_items / num_parts) if total_items > 0 else 0
    if total_items > 0:
        print(f"  üîÄ Redistributing into {num_parts} parts (~{items_per_part} items each)")

    # Convert to list for easier slicing
    items = list(filtered_data.items())

    # Distribute into parts
    for i, part_file in enumerate(existing_parts):
        part_num = i + 1
        start_idx = i * items_per_part
        end_idx = min((i + 1) * items_per_part, total_items) if total_items > 0 else 0

        if start_idx >= total_items:
            # No more items, but we still need to write an empty file
            part_data = OrderedDict()
        else:
            part_data = OrderedDict(items[start_idx:end_idx])

        # Load existing status (resolved/comment) from the part file for items still present
        existing_status = load_existing_part_file(part_file)

        # Write the refreshed file
        write_redistributed_file(
            part_file,
            part_data,
            file_type,
            part_num,
            num_parts,
            total_items,
            existing_status
        )

        print(f"    ‚úÖ Refreshed: {part_file.name} ({len(part_data)} items)")

    print(f"  üìÅ All parts refreshed in: {redistrib_dir}/")
    return num_parts


def redistribute_all_files(output_dir):
    """Redistribute all collector output files into existing folders."""
    print("\n" + "=" * 70)
    print("STEP 2: Refreshing existing redistributed files...")
    print("=" * 70)

    files_to_redistribute = [
        {
            'input': os.path.join(output_dir, "multiple_dhatu_ids_without_gati.yaml"),
            'basename': "multipleDhatuIdsWithoutGati",
            'type': "Multiple dhatu_ids (WITHOUT gati)"
        },
        {
            'input': os.path.join(output_dir, "multiple_dhatu_ids_with_gati.yaml"),
            'basename': "multipleDhatuIdsWithGati",
            'type': "Multiple dhatu_ids (WITH gati)"
        },
        {
            'input': os.path.join(output_dir, "not_found_dhatu_ids_without_gati.yaml"),
            'basename': "notFoundDhatuIdsWithoutGati",
            'type': "Not Found dhatu_ids (WITHOUT gati)"
        }
    ]

    total_parts_refreshed = 0

    for file_info in files_to_redistribute:
        parts = redistribute_file(
            file_info['input'],
            output_dir,
            file_info['basename'],
            file_info['type']
        )
        total_parts_refreshed += parts

    print(f"\n{'=' * 70}")
    print(f"‚úÖ Redistribution complete! Refreshed {total_parts_refreshed} total files")
    print(f"{'=' * 70}")


def main():
    parser = argparse.ArgumentParser(
        description='Refresh collector outputs and redistribute work into existing files',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        'json_file',
        help='Path to the generated JSON file'
    )

    parser.add_argument(
        'output_dir',
        help='Directory where output files are located'
    )

    parser.add_argument(
        '--skip-refresh',
        action='store_true',
        help='Skip running collectors, only redistribute existing files'
    )

    args = parser.parse_args()

    # Validate inputs
    if not os.path.exists(args.json_file):
        print(f"‚ùå Error: JSON file not found: {args.json_file}")
        sys.exit(1)

    if not os.path.exists(args.output_dir):
        print(f"‚ùå Error: Output directory not found: {args.output_dir}")
        sys.exit(1)

    print("\n" + "=" * 70)
    print("REFRESH AND REDISTRIBUTE WORK SCRIPT")
    print("=" * 70)
    print(f"JSON file: {args.json_file}")
    print(f"Output directory: {args.output_dir}")

    # Step 1: Run collectors (unless skipped)
    if not args.skip_refresh:
        if not run_collectors(args.json_file, args.output_dir):
            print("\n‚ùå Failed to refresh collector files")
            sys.exit(1)
    else:
        print("\n‚è≠Ô∏è  Skipping collector refresh (--skip-refresh flag)")

    # Step 2: Redistribute files into existing folders
    redistribute_all_files(args.output_dir)

    print("\n" + "=" * 70)
    print("üéâ ALL DONE!")
    print("=" * 70)
    print("\nNext steps:")
    print("  1. Workers can continue editing their assigned part files")
    print("  2. The files have been refreshed with latest data while preserving resolved status")
    print("  3. Run the backport script when ready to sync changes back to original YAML files")


if __name__ == "__main__":
    main()
